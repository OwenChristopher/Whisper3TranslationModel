{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet torch whisper pydub noisereduce soundfile speechbrain transformers pyttsx3 langchain langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from pydub import AudioSegment, effects\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "from speechbrain.inference import EncoderClassifier\n",
    "import pyttsx3\n",
    "import sounddevice as sd\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langdetect import detect, DetectorFactory, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def setup_tts():\n",
    "    tts_engine.setProperty('rate', 130)    # Speed percent (can go over 100)\n",
    "    tts_engine.setProperty('volume', 0.9)  # Volume 0-1\n",
    "    voices = tts_engine.getProperty('voices')\n",
    "    tts_engine.setProperty('voice', voices[0].id)  # 0 for male, 1 for female\n",
    "\n",
    "def speak_text(text):\n",
    "    tts_engine.say(text)\n",
    "    tts_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_text(\"hello there friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = whisper.load_model(\"large\").to(device)  # Options: ['base', 'small', 'medium', 'large']\n",
    "print(\"Whisper model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_settings():\n",
    "    print(\"Welcome to the Translation App!\")\n",
    "    objective = input(\"Please enter your objective (e.g., Translate a conversation): \").strip()\n",
    "    target_language = input(\"Please enter the target language (e.g., Spanish, French): \").strip()\n",
    "    return objective, target_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(input_path, output_path, target_dBFS=-20.0):\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    normalized_audio = effects.normalize(audio, headroom=target_dBFS)\n",
    "    normalized_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Normalized audio saved to {output_path}\")\n",
    "\n",
    "def reduce_noise(input_path, output_path):\n",
    "    data, rate = sf.read(input_path)\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "    sf.write(output_path, reduced_noise, rate)\n",
    "    print(f\"Noise-reduced audio saved to {output_path}\")\n",
    "\n",
    "def preprocess_audio(input_path, normalized_path, denoised_path):\n",
    "    normalize_audio(input_path, normalized_path)\n",
    "    reduce_noise(normalized_path, denoised_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_whisper(audio_path):\n",
    "    print(\"Transcribing with Whisper...\")\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_language_model():\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent(llm, text):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant that extracts the user's intent from their input.\n",
    "\n",
    "User Input: \"{text}\"\n",
    "\n",
    "Determine the user's intent and extract relevant information.\n",
    "\n",
    "Intent and Information:\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run(text=text)\n",
    "    return response.strip()\n",
    "\n",
    "# Generate assistant response using the LLM\n",
    "def generate_response(llm, intent_info):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"intent_info\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant tasked with helping the user achieve their objectives based on the following intent and information.\n",
    "\n",
    "Intent and Information:\n",
    "{intent_info}\n",
    "\n",
    "Generate a response that moves towards achieving the user's objective.\n",
    "\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run(intent_info=intent_info)\n",
    "    return response.strip()\n",
    "\n",
    "class SessionState:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.objectives = {}\n",
    "\n",
    "    def add_interaction(self, user_text, assistant_response):\n",
    "        self.history.append({\"user\": user_text, \"assistant\": assistant_response})\n",
    "\n",
    "    def set_objective(self, objective, details):\n",
    "        self.objectives[objective] = details\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def get_objectives(self):\n",
    "        return self.objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(llm, session):\n",
    "    history = session.get_history()\n",
    "    summary_text = \"Conversation History:\\n\"\n",
    "    for interaction in history:\n",
    "        summary_text += f\"User: {interaction['user']}\\nAssistant: {interaction['assistant']}\\n\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant that summarizes conversations.\n",
    "\n",
    "{history}\n",
    "\n",
    "Generate a concise summary of the conversation, focusing on the objectives achieved and any unresolved issues.\n",
    "\n",
    "Summary:\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    summary = chain.run(history=summary_text)\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized audio saved to normalized_audio.wav\n",
      "Noise-reduced audio saved to denoised_audio.wav\n",
      "Transcribing with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Whisper Transcription:**\n",
      " Hello, this is a very nice test. Can you understand me MMS?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\inspect.py:1001: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_58084\\3541345331.py:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_58084\\3541345331.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(text=text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Recognized Intent and Information:**\n",
      "## Intent and Information:\n",
      "\n",
      "**Intent:**  The user is testing the assistant's ability to understand natural language.\n",
      "\n",
      "**Information:**\n",
      "\n",
      "* The user is using a friendly greeting (\"Hello\").\n",
      "* The user is providing a positive evaluation of the test (\"this is a very nice test\").\n",
      "* The user is explicitly asking if the assistant can understand them (\"Can you understand me\").\n",
      "* The user is using an abbreviation (\"MMS\") which could be interpreted as \"Multi-Media Service\" or just a random string of letters. However, without context, it's difficult to determine the exact meaning.\n",
      "\n",
      "**Assistant Response:**\n",
      "Hello!  It's great to hear you think this is a nice test. 😊  And yes, I can understand you!  \n",
      "\n",
      "I'm still learning, so I'm not always perfect.  Could you tell me a little more about what you meant by \"MMS\"?  That will help me understand you even better.\n",
      "\n",
      "**Summary:**\n",
      "Summary:\n",
      "\n",
      "The user initiated the conversation by expressing positive feedback about the test. The assistant acknowledged the feedback and confirmed its ability to understand the user. The user then used the acronym \"MMS,\" which the assistant did not understand. The assistant requested clarification to improve its understanding. \n",
      "\n",
      "**Objectives Achieved:** \n",
      "* The assistant established a positive interaction with the user.\n",
      "* The assistant confirmed its ability to understand the user's basic statements.\n",
      "\n",
      "**Unresolved Issues:**\n",
      "* The meaning of \"MMS\" in the user's context remains unclear.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    setup_tts()\n",
    "    llm = initialize_language_model()\n",
    "    session = SessionState()\n",
    "\n",
    "    # Paths to your audio files\n",
    "    input_audio = \"testing.wav\"  # Update with your input audio path\n",
    "    normalized_audio = \"normalized_audio.wav\"\n",
    "    denoised_audio = \"denoised_audio.wav\"\n",
    "\n",
    "    preprocess_audio(input_audio, normalized_audio, denoised_audio)\n",
    "\n",
    "    whisper_text = transcribe_whisper(\"denoised_audio.wav\")\n",
    "    print(\"\\n**Whisper Transcription:**\")\n",
    "    print(whisper_text)\n",
    "    speak_text(whisper_text)  \n",
    "\n",
    "    intent_info = recognize_intent(llm, whisper_text)\n",
    "    print(\"\\n**Recognized Intent and Information:**\")\n",
    "    print(intent_info)\n",
    "\n",
    "    assistant_response = generate_response(llm, intent_info)\n",
    "    print(\"\\n**Assistant Response:**\")\n",
    "    print(assistant_response)\n",
    "    speak_text(assistant_response)  # Speak the assistant's response\n",
    "\n",
    "    session.add_interaction(whisper_text, assistant_response)\n",
    "\n",
    "    # Check if an objective is achieved or if there is trouble\n",
    "    # For demonstration, we'll generate a summary after each interaction\n",
    "    summary = generate_summary(llm, session)\n",
    "    print(\"\\n**Summary:**\")\n",
    "    print(summary)\n",
    "    speak_text(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-1.5-flash\",\n",
    "#     google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "#     temperature=0.5\n",
    "# )\n",
    "\n",
    "# result = MODEL.invoke(\"Write a ballad about LangChain\")\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\cuda\\__init__.py:435\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    424\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\cuda\\__init__.py:465\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
