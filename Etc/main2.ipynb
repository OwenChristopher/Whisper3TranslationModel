{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from pydub import AudioSegment, effects\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "from speechbrain.inference import EncoderClassifier\n",
    "import pyttsx3\n",
    "import sounddevice as sd\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langdetect import detect, DetectorFactory, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionState:\n",
    "    def __init__(self, objective, target_language):\n",
    "        self.objective = objective\n",
    "        self.target_language = target_language\n",
    "        self.history = []  # List of dictionaries: {'user': ..., 'assistant': ...}\n",
    "        self.current_status = 'ongoing'  # Can be 'ongoing', 'fulfilled', 'failed'\n",
    "\n",
    "    def add_interaction(self, user_text, assistant_response):\n",
    "        self.history.append({\"user\": user_text, \"assistant\": assistant_response})\n",
    "\n",
    "    def update_status(self, status):\n",
    "        self.current_status = status\n",
    "\n",
    "    def get_summary(self):\n",
    "        return \"\\n\".join([f\"User: {item['user']}\\nAssistant: {item['assistant']}\" for item in self.history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioHandler:\n",
    "    def __init__(self, sample_rate=16000, duration=5):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "\n",
    "    def record_audio(self):\n",
    "        print(\"Recording...\")\n",
    "        try:\n",
    "            recording = sd.rec(int(self.duration * self.sample_rate), samplerate=self.sample_rate, channels=1, dtype='float32')\n",
    "            sd.wait()  # Wait until recording is finished\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n",
    "                sf.write(tmpfile.name, recording, self.sample_rate)\n",
    "                print(f\"Audio recorded and saved to {tmpfile.name}\")\n",
    "                return tmpfile.name\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to record audio: {e}\")\n",
    "            return None\n",
    "\n",
    "    def preprocess_audio(self, input_path):\n",
    "        normalized_path = \"normalized_audio.wav\"\n",
    "        denoised_path = \"denoised_audio.wav\"\n",
    "        \n",
    "        # Normalize Audio\n",
    "        audio = AudioSegment.from_file(input_path)\n",
    "        normalized_audio = effects.normalize(audio, headroom=-20.0)\n",
    "        normalized_audio.export(normalized_path, format=\"wav\")\n",
    "        print(f\"Normalized audio saved to {normalized_path}\")\n",
    "\n",
    "        # Reduce Noise\n",
    "        data, rate = sf.read(normalized_path)\n",
    "        reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "        sf.write(denoised_path, reduced_noise, rate)\n",
    "        print(f\"Noise-reduced audio saved to {denoised_path}\")\n",
    "\n",
    "        return denoised_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionHandler:\n",
    "    def __init__(self, model_name='large', device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(\"Loading Whisper model...\")\n",
    "        self.model = whisper.load_model(model_name).to(self.device)\n",
    "        print(\"Whisper model loaded.\")\n",
    "\n",
    "    def transcribe(self, audio_path):\n",
    "        print(\"Transcribing with Whisper...\")\n",
    "        result = self.model.transcribe(audio_path)\n",
    "        return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm  # Instance of ChatGoogleGenerativeAI or similar\n",
    "\n",
    "    def detect_language(self, text):\n",
    "        try:\n",
    "            language = detect(text)\n",
    "            print(f\"Detected language: {language}\")\n",
    "            return language\n",
    "        except LangDetectException:\n",
    "            print(\"Could not detect language.\")\n",
    "            return None\n",
    "\n",
    "    def translate_text(self, text, source_lang, target_lang):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"source_lang\", \"target_lang\"],\n",
    "            template=\"\"\"\n",
    "You are a proficient translator.\n",
    "\n",
    "Source Language: {source_lang}\n",
    "Target Language: {target_lang}\n",
    "\n",
    "Please translate the following text from {source_lang} to {target_lang}:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "Translation:\n",
    "\"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
    "        translation = chain.run(text=text, source_lang=source_lang, target_lang=target_lang)\n",
    "        return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentRecognizer:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def recognize_intent(self, text):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "You are an assistant that extracts the user's intent from their input.\n",
    "\n",
    "User Input: \"{text}\"\n",
    "\n",
    "Determine the user's intent and extract relevant information.\n",
    "\n",
    "Intent and Information:\n",
    "\"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
    "        response = chain.run(text=text)\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self, llm, session):\n",
    "        self.llm = llm\n",
    "        self.session = session\n",
    "        self.intent_recognizer = IntentRecognizer(llm)\n",
    "        self.language_processor = LanguageProcessor(llm)\n",
    "\n",
    "    def evaluate_objective(self, user_text, assistant_response):\n",
    "        # Define how to evaluate if the objective is met\n",
    "        # This could be based on keywords, specific responses, or a more complex analysis\n",
    "        # For simplicity, let's assume if the assistant confirms the objective, it's fulfilled\n",
    "        if any(keyword in assistant_response.lower() for keyword in [\"completed\", \"achieved\", \"done\"]):\n",
    "            self.session.update_status('fulfilled')\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def generate_response(self, translated_text):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"translated_text\", \"objective\"],\n",
    "            template=\"\"\"\n",
    "You are an assistant tasked with helping the user achieve their objective.\n",
    "\n",
    "Objective: {objective}\n",
    "\n",
    "User Input: \"{translated_text}\"\n",
    "\n",
    "Generate a response that moves towards achieving the user's objective.\n",
    "\"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
    "        response = chain.run(translated_text=translated_text, objective=self.session.objective)\n",
    "        return response.strip()\n",
    "\n",
    "    def manage_conversation(self, user_text):\n",
    "        # Detect language\n",
    "        source_lang = self.language_processor.detect_language(user_text)\n",
    "        if not source_lang:\n",
    "            return \"Sorry, I couldn't detect the language of your input.\", False\n",
    "\n",
    "        # Translate to target language\n",
    "        translated_text = self.language_processor.translate_text(user_text, source_lang, self.session.target_language)\n",
    "        print(f\"Translated Text: {translated_text}\")\n",
    "\n",
    "        # Recognize intent (if needed)\n",
    "        intent_info = self.intent_recognizer.recognize_intent(translated_text)\n",
    "        print(f\"Recognized Intent: {intent_info}\")\n",
    "\n",
    "        # Generate assistant response\n",
    "        assistant_response = self.generate_response(translated_text)\n",
    "        print(f\"Assistant Response: {assistant_response}\")\n",
    "\n",
    "        # Optionally, translate assistant response back to user's language\n",
    "        final_response = self.language_processor.translate_text(assistant_response, self.session.target_language, source_lang)\n",
    "        print(f\"Final Response (Translated Back): {final_response}\")\n",
    "\n",
    "        # Add to session history\n",
    "        self.session.add_interaction(user_text, final_response)\n",
    "\n",
    "        # Evaluate if objective is met\n",
    "        if self.evaluate_objective(user_text, assistant_response):\n",
    "            return final_response, True  # Objective fulfilled\n",
    "        else:\n",
    "            return final_response, False  # Continue conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate_summary(self, history):\n",
    "        summary_text = \"Conversation History:\\n\" + \"\\n\".join(\n",
    "            [f\"User: {item['user']}\\nAssistant: {item['assistant']}\" for item in history]\n",
    "        )\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"history\"],\n",
    "            template=\"\"\"\n",
    "You are an assistant that summarizes conversations.\n",
    "\n",
    "{history}\n",
    "\n",
    "Generate a concise summary of the conversation, focusing on the objectives achieved and any unresolved issues.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
    "        summary = chain.run(history=summary_text)\n",
    "        return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function for User Settings\n",
    "LANGUAGE_CODE_MAP = {\n",
    "    'english': 'en',\n",
    "    'spanish': 'es',\n",
    "    'french': 'fr',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def get_user_settings():\n",
    "    print(\"Welcome to the Translation App!\")\n",
    "    objective = input(\"Please enter your objective (e.g., Schedule a meeting): \").strip()\n",
    "    target_language_input = input(\"Please enter the target language (e.g., English, Spanish): \").strip().lower()\n",
    "    target_language = LANGUAGE_CODE_MAP.get(target_language_input, 'en')  # Default to English\n",
    "    return objective, target_language\n",
    "\n",
    "# %%\n",
    "# Language Model Initialization\n",
    "def initialize_language_model():\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Translation App!\n",
      "Using device: cpu\n",
      "Loading Whisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded.\n",
      "Objective set to: Negotiate price of taxi to 4 dollars\n",
      "Target language: es\n",
      "\n",
      "Please speak your input. Press Ctrl+C to exit.\n",
      "Recording...\n",
      "Audio recorded and saved to C:\\Users\\Owenc\\AppData\\Local\\Temp\\tmpj848l1vv.wav\n",
      "Normalized audio saved to normalized_audio.wav\n",
      "Noise-reduced audio saved to denoised_audio.wav\n",
      "Transcribing with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Transcribed Text:**  Продолжение следует...\n",
      "Detected language: ru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\inspect.py:1001: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_55792\\2976721085.py:30: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_55792\\2976721085.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  translation = chain.run(text=text, source_lang=source_lang, target_lang=target_lang)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: \"Continuará...\"\n",
      "Recognized Intent: ## Intent and Information:\n",
      "\n",
      "**Intent:**  To indicate that the current story, conversation, or event is not finished and will continue at a later time.\n",
      "\n",
      "**Information:**  The user is using a common phrase in Spanish, \"Continuará...\", which translates to \"To be continued...\" in English. This implies a continuation of the current topic.\n",
      "Assistant Response: I understand you want to negotiate the price of a taxi down to $4.  \"Continuará...\" doesn't give me much to work with.  To help you, I need more information! \n",
      "\n",
      "Tell me:\n",
      "\n",
      "* **Where are you?** (Country, city, or even just a general location helps)\n",
      "* **What is the current price the taxi driver is asking?**\n",
      "* **What is your destination?** (This can help me understand if the price is reasonable)\n",
      "* **What is your strategy for negotiating?** (Are you friendly and polite, or are you more assertive?)\n",
      "\n",
      "Once I have this information, I can give you specific advice on how to negotiate the price down to $4.\n",
      "Final Response (Translated Back): Понимаю, что вы хотите договориться о цене такси до 4 долларов. \"Продолжение следует...\" не дает мне много информации для работы. Чтобы помочь вам, мне нужна дополнительная информация!\n",
      "\n",
      "Скажите мне:\n",
      "\n",
      "* **Где вы находитесь?** (Страна, город или даже просто общее местоположение помогут)\n",
      "* **Какая текущая цена, которую просит водитель такси?**\n",
      "* **Куда вы едете?** (Это поможет мне понять, является ли цена разумной)\n",
      "* **Какова ваша стратегия ведения переговоров?** (Вы дружелюбны и вежливы, или вы более напористы?)\n",
      "\n",
      "Как только я получу эту информацию, я смогу дать вам конкретные советы о том, как договориться о цене до 4 долларов.\n",
      "**Assistant Response:** Понимаю, что вы хотите договориться о цене такси до 4 долларов. \"Продолжение следует...\" не дает мне много информации для работы. Чтобы помочь вам, мне нужна дополнительная информация!\n",
      "\n",
      "Скажите мне:\n",
      "\n",
      "* **Где вы находитесь?** (Страна, город или даже просто общее местоположение помогут)\n",
      "* **Какая текущая цена, которую просит водитель такси?**\n",
      "* **Куда вы едете?** (Это поможет мне понять, является ли цена разумной)\n",
      "* **Какова ваша стратегия ведения переговоров?** (Вы дружелюбны и вежливы, или вы более напористы?)\n",
      "\n",
      "Как только я получу эту информацию, я смогу дать вам конкретные советы о том, как договориться о цене до 4 долларов.\n",
      "\n",
      "Please speak your input. Press Ctrl+C to exit.\n",
      "Recording...\n",
      "Audio recorded and saved to C:\\Users\\Owenc\\AppData\\Local\\Temp\\tmpbcf9j_hf.wav\n",
      "Normalized audio saved to normalized_audio.wav\n",
      "Noise-reduced audio saved to denoised_audio.wav\n",
      "Transcribing with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Transcribed Text:**  Oh\n",
      "Detected language: de\n",
      "Translated Text: \"Ah\"\n",
      "Recognized Intent: ## Intent and Information:\n",
      "\n",
      "**Intent:**  It's difficult to determine a clear intent from the input \"Ah\". It could be interpreted in several ways, depending on context:\n",
      "\n",
      "* **Acknowledgement:** The user might be acknowledging something previously said.\n",
      "* **Surprise:** The user might be expressing surprise or astonishment.\n",
      "* **Disappointment:** The user might be expressing disappointment or frustration.\n",
      "* **Pain:** The user might be expressing pain or discomfort.\n",
      "* **Hesitation:** The user might be hesitating before continuing a thought.\n",
      "\n",
      "**Information:** There is no relevant information to extract from this input alone. \n",
      "\n",
      "**Recommendation:**  To understand the user's intent, more context is needed. For example, you could ask:\n",
      "\n",
      "* \"What's that about?\"\n",
      "* \"What happened?\"\n",
      "* \"Can you tell me more?\"\n",
      "Assistant Response: \"Ah, that's a bit steep! Could you do $4?\"\n",
      "Final Response (Translated Back): \"Ach, das ist etwas zu teuer! Könntest du es für 4 Dollar machen?\"\n",
      "**Assistant Response:** \"Ach, das ist etwas zu teuer! Könntest du es für 4 Dollar machen?\"\n",
      "\n",
      "Please speak your input. Press Ctrl+C to exit.\n",
      "Recording...\n",
      "Audio recorded and saved to C:\\Users\\Owenc\\AppData\\Local\\Temp\\tmpaift00j8.wav\n",
      "Normalized audio saved to normalized_audio.wav\n",
      "Noise-reduced audio saved to denoised_audio.wav\n",
      "Transcribing with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation terminated by user.\n",
      "\n",
      "**Conversation Summary:**\n",
      "The user is attempting to negotiate a taxi fare down to $4. The assistant is requesting more information to provide helpful advice, but the user has not provided any details about their location, the current price, their destination, or their negotiation style. The conversation ends with the user simply saying \"Oh\", leaving the negotiation unresolved.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize Language Model\n",
    "    try:\n",
    "        llm = initialize_language_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize language model: {e}\")\n",
    "        # Handle the error appropriately (e.g., notify the user)\n",
    "        return\n",
    "\n",
    "    # Get User Settings\n",
    "    try:\n",
    "        objective, target_language = get_user_settings()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get user settings: {e}\")\n",
    "        # Handle the error appropriately\n",
    "        return\n",
    "\n",
    "    # Initialize Session State\n",
    "    session = SessionState(objective=objective, target_language=target_language)\n",
    "\n",
    "    # Initialize Conversation Manager\n",
    "    conversation_manager = ConversationManager(llm=llm, session=session)\n",
    "\n",
    "    # Initialize Audio Handler\n",
    "    audio_handler = AudioHandler(duration=5, sample_rate=16000)\n",
    "\n",
    "    # Initialize Transcription Handler\n",
    "    transcription_handler = TranscriptionHandler()\n",
    "\n",
    "    # Initialize Summary Generator\n",
    "    summary_generator = SummaryGenerator(llm=llm)\n",
    "\n",
    "    print(f\"Objective set to: {objective}\")\n",
    "    print(f\"Target language: {target_language}\")\n",
    "\n",
    "    while session.current_status == 'ongoing':\n",
    "        print(\"\\nPlease speak your input. Press Ctrl+C to exit.\")\n",
    "        try:\n",
    "            # Record Audio\n",
    "            input_audio = audio_handler.record_audio()\n",
    "            if not input_audio:\n",
    "                print(\"Failed to record audio. Please try again.\")\n",
    "                continue\n",
    "\n",
    "            # Preprocess Audio\n",
    "            denoised_audio = audio_handler.preprocess_audio(input_audio)\n",
    "\n",
    "            # Transcribe Audio\n",
    "            user_text = transcription_handler.transcribe(denoised_audio)\n",
    "            print(f\"**Transcribed Text:** {user_text}\")\n",
    "\n",
    "            # Manage Conversation\n",
    "            assistant_response, fulfilled = conversation_manager.manage_conversation(user_text)\n",
    "            print(f\"**Assistant Response:** {assistant_response}\")\n",
    "\n",
    "            # Optionally, implement TTS here later\n",
    "\n",
    "            if fulfilled:\n",
    "                print(\"Objective fulfilled. Ending conversation.\")\n",
    "                session.update_status('fulfilled')\n",
    "                break\n",
    "\n",
    "            # Cleanup temporary audio files\n",
    "            os.remove(input_audio)\n",
    "            os.remove(denoised_audio)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nConversation terminated by user.\")\n",
    "            session.update_status('failed')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            # Handle other exceptions appropriately\n",
    "\n",
    "    # Generate Summary if Conversation Ended\n",
    "    if session.history:\n",
    "        summary = summary_generator.generate_summary(session.history)\n",
    "        print(\"\\n**Conversation Summary:**\")\n",
    "        print(summary)\n",
    "        # Optionally, implement TTS for summary here later\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
