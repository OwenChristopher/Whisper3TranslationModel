{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet torch whisper pydub noisereduce soundfile speechbrain transformers pyttsx3 langchain langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from pydub import AudioSegment, effects\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "from speechbrain.inference import EncoderClassifier\n",
    "import pyttsx3\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def setup_tts():\n",
    "    tts_engine.setProperty('rate', 130)    # Speed percent (can go over 100)\n",
    "    tts_engine.setProperty('volume', 0.9)  # Volume 0-1\n",
    "    voices = tts_engine.getProperty('voices')\n",
    "    tts_engine.setProperty('voice', voices[0].id)  # 0 for male, 1 for female\n",
    "\n",
    "def speak_text(text):\n",
    "    tts_engine.say(text)\n",
    "    tts_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_text(\"hello there friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = whisper.load_model(\"large\").to(device)  # Options: ['base', 'small', 'medium', 'large']\n",
    "print(\"Whisper model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(input_path, output_path, target_dBFS=-20.0):\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    normalized_audio = effects.normalize(audio, headroom=target_dBFS)\n",
    "    normalized_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Normalized audio saved to {output_path}\")\n",
    "\n",
    "def reduce_noise(input_path, output_path):\n",
    "    data, rate = sf.read(input_path)\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "    sf.write(output_path, reduced_noise, rate)\n",
    "    print(f\"Noise-reduced audio saved to {output_path}\")\n",
    "\n",
    "def preprocess_audio(input_path, normalized_path, denoised_path):\n",
    "    normalize_audio(input_path, normalized_path)\n",
    "    reduce_noise(normalized_path, denoised_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_whisper(audio_path):\n",
    "    print(\"Transcribing with Whisper...\")\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_language_model():\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent(llm, text):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant that extracts the user's intent from their input.\n",
    "\n",
    "User Input: \"{text}\"\n",
    "\n",
    "Determine the user's intent and extract relevant information.\n",
    "\n",
    "Intent and Information:\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run(text=text)\n",
    "    return response.strip()\n",
    "\n",
    "# Generate assistant response using the LLM\n",
    "def generate_response(llm, intent_info):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"intent_info\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant tasked with helping the user achieve their objectives based on the following intent and information.\n",
    "\n",
    "Intent and Information:\n",
    "{intent_info}\n",
    "\n",
    "Generate a response that moves towards achieving the user's objective.\n",
    "\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run(intent_info=intent_info)\n",
    "    return response.strip()\n",
    "\n",
    "class SessionState:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.objectives = {}\n",
    "\n",
    "    def add_interaction(self, user_text, assistant_response):\n",
    "        self.history.append({\"user\": user_text, \"assistant\": assistant_response})\n",
    "\n",
    "    def set_objective(self, objective, details):\n",
    "        self.objectives[objective] = details\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def get_objectives(self):\n",
    "        return self.objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(llm, session):\n",
    "    history = session.get_history()\n",
    "    summary_text = \"Conversation History:\\n\"\n",
    "    for interaction in history:\n",
    "        summary_text += f\"User: {interaction['user']}\\nAssistant: {interaction['assistant']}\\n\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\"],\n",
    "        template=\"\"\"\n",
    "You are an assistant that summarizes conversations.\n",
    "\n",
    "{history}\n",
    "\n",
    "Generate a concise summary of the conversation, focusing on the objectives achieved and any unresolved issues.\n",
    "\n",
    "Summary:\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    summary = chain.run(history=summary_text)\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized audio saved to normalized_audio.wav\n",
      "Noise-reduced audio saved to denoised_audio.wav\n",
      "Transcribing with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Whisper Transcription:**\n",
      " Hello, this is a very nice test. Can you understand me MMS?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owenc\\anaconda3\\envs\\hackathon\\Lib\\inspect.py:1001: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_58084\\3541345331.py:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "C:\\Users\\Owenc\\AppData\\Local\\Temp\\ipykernel_58084\\3541345331.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(text=text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Recognized Intent and Information:**\n",
      "## Intent and Information:\n",
      "\n",
      "**Intent:**  The user is testing the assistant's ability to understand natural language.\n",
      "\n",
      "**Information:**\n",
      "\n",
      "* The user is using a friendly greeting (\"Hello\").\n",
      "* The user is providing a positive evaluation of the test (\"this is a very nice test\").\n",
      "* The user is explicitly asking if the assistant can understand them (\"Can you understand me\").\n",
      "* The user is using an abbreviation (\"MMS\") which could be interpreted as \"Multi-Media Service\" or just a random string of letters. However, without context, it's difficult to determine the exact meaning.\n",
      "\n",
      "**Assistant Response:**\n",
      "Hello!  It's great to hear you think this is a nice test. 😊  And yes, I can understand you!  \n",
      "\n",
      "I'm still learning, so I'm not always perfect.  Could you tell me a little more about what you meant by \"MMS\"?  That will help me understand you even better.\n",
      "\n",
      "**Summary:**\n",
      "Summary:\n",
      "\n",
      "The user initiated the conversation by expressing positive feedback about the test. The assistant acknowledged the feedback and confirmed its ability to understand the user. The user then used the acronym \"MMS,\" which the assistant did not understand. The assistant requested clarification to improve its understanding. \n",
      "\n",
      "**Objectives Achieved:** \n",
      "* The assistant established a positive interaction with the user.\n",
      "* The assistant confirmed its ability to understand the user's basic statements.\n",
      "\n",
      "**Unresolved Issues:**\n",
      "* The meaning of \"MMS\" in the user's context remains unclear.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    setup_tts()\n",
    "    llm = initialize_language_model()\n",
    "    session = SessionState()\n",
    "\n",
    "    # Paths to your audio files\n",
    "    input_audio = \"testing.wav\"  # Update with your input audio path\n",
    "    normalized_audio = \"normalized_audio.wav\"\n",
    "    denoised_audio = \"denoised_audio.wav\"\n",
    "\n",
    "    preprocess_audio(input_audio, normalized_audio, denoised_audio)\n",
    "\n",
    "    whisper_text = transcribe_whisper(\"denoised_audio.wav\")\n",
    "    print(\"\\n**Whisper Transcription:**\")\n",
    "    print(whisper_text)\n",
    "    speak_text(whisper_text)  \n",
    "\n",
    "    intent_info = recognize_intent(llm, whisper_text)\n",
    "    print(\"\\n**Recognized Intent and Information:**\")\n",
    "    print(intent_info)\n",
    "\n",
    "    assistant_response = generate_response(llm, intent_info)\n",
    "    print(\"\\n**Assistant Response:**\")\n",
    "    print(assistant_response)\n",
    "    speak_text(assistant_response)  # Speak the assistant's response\n",
    "\n",
    "    session.add_interaction(whisper_text, assistant_response)\n",
    "\n",
    "    # Check if an objective is achieved or if there is trouble\n",
    "    # For demonstration, we'll generate a summary after each interaction\n",
    "    summary = generate_summary(llm, session)\n",
    "    print(\"\\n**Summary:**\")\n",
    "    print(summary)\n",
    "    speak_text(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In realms of code, where data flows,\n",
      "A knight of knowledge, LangChain grows.\n",
      "With chains of thought, it weaves a spell,\n",
      "To unlock secrets, stories to tell.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a tool so grand,\n",
      "Connecting models, hand in hand.\n",
      "From LLMs vast, to memories stored,\n",
      "It builds a bridge, where wisdom is poured.\n",
      "\n",
      "(Verse 2)\n",
      "With prompts it whispers, questions it asks,\n",
      "To chains of reasoning, it gently tasks.\n",
      "From simple queries, to complex schemes,\n",
      "It finds the answers, fulfilling dreams.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a tool so grand,\n",
      "Connecting models, hand in hand.\n",
      "From LLMs vast, to memories stored,\n",
      "It builds a bridge, where wisdom is poured.\n",
      "\n",
      "(Verse 3)\n",
      "It gathers facts, from sources diverse,\n",
      "And weaves them into a narrative verse.\n",
      "With agents it acts, in the real world's fray,\n",
      "Solving problems, in a clever way.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a tool so grand,\n",
      "Connecting models, hand in hand.\n",
      "From LLMs vast, to memories stored,\n",
      "It builds a bridge, where wisdom is poured.\n",
      "\n",
      "(Verse 4)\n",
      "So sing a ballad, of LangChain's might,\n",
      "A beacon of knowledge, shining bright.\n",
      "With every chain, a new path unfolds,\n",
      "A future of learning, stories untold. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = MODEL.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
